{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sample(sample):\n",
    "    d = sample\n",
    "    minX = np.min(d[:,0])\n",
    "    maxX = np.max(d[:,0])\n",
    "    minY = np.min(d[:,1])\n",
    "    maxY = np.max(d[:,1])\n",
    "    d[:,0] = (d[:,0] - minX) / (maxX - minX)\n",
    "    d[:,1] = (d[:,1] - minY) / (maxY - minY)   \n",
    "    return d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "\n",
    "    # Normalize the data\n",
    "    data = normalize_sample(data)\n",
    "\n",
    "    # Remove data points that are too close to each other\n",
    "    data_points = [data[0,:]]\n",
    "    for measurement in data[1:]:\n",
    "        if np.linalg.norm(measurement[0:2] - data_points[-1][0:2]) > 0.05:\n",
    "            data_points.append(measurement)\n",
    "\n",
    "    data = normalize_sample(np.array(data_points))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(data):\n",
    "\n",
    "    features = []\n",
    "\n",
    "    N = np.shape(data)[0]\n",
    "\n",
    "    xy_dist_from_origin = [np.sum(data[i,0:2]**2)**0.5 for i in range(N)]\n",
    "    features.append(np.max(xy_dist_from_origin)-np.min(xy_dist_from_origin)) # max-min distance from origin\n",
    "\n",
    "    dist_end_to_start = np.linalg.norm(data[-1] - data[0])\n",
    "    features.append(dist_end_to_start) # distance from end to start\n",
    "\n",
    "    features.append(np.std(data[:,0], ddof=0)) # standard deviation of x\n",
    "    features.append(np.std(data[:,1], ddof=0)) # standard deviation of y\n",
    "    features.append(np.mean(data[:,0])) # average of x\n",
    "    features.append(np.mean(data[:,1])) # average of y\n",
    "\n",
    "    xy_dist_from_mean = [np.sum((data[i,0:2] - np.mean(data[i,0:2]))**2)**0.5 for i in range(N)]\n",
    "    features.append(np.std(xy_dist_from_mean, ddof=0)) # deviation of distance from mean\n",
    "    features.append(np.mean(xy_dist_from_mean)) # average distance from mean\n",
    "\n",
    "    \n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_creation(stroke):\n",
    "    # This function assumes that the stroke is already normalized\n",
    "    D = 20\n",
    "    M = np.zeros([D,D])\n",
    "    for i,point in enumerate(stroke[1:,0:2]):\n",
    "        x0, y0 = stroke[i,0:2]\n",
    "        x1, y1 = point\n",
    "        x0 = int(x0*(D-1))\n",
    "        y0 = int(y0*(D-1))\n",
    "        x1 = int(x1*(D-1))\n",
    "        y1 = int(y1*(D-1))\n",
    "\n",
    "        dist = int(2*np.linalg.norm([x1-x0, y1-y0]))\n",
    "        xspan = np.linspace(x0, x1, dist)\n",
    "        yspan = np.linspace(y0, y1, dist)\n",
    "        for i in range(dist):\n",
    "            M[int(xspan[i]),int(yspan[i])] = 1\n",
    "    return np.flipud(M.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x111f644c4d0>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa4AAAGdCAYAAABKG5eZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAj5ElEQVR4nO3df2xV9f3H8dct0Ftn6K0KtL1Syg8FNoTOkbUrzvlVOkrnEHRTbNwoE3UhJZlhLoxlrmwmqw63P4YNmilU41Q0kZKow0Hlx5BWJm0zUNdQVlsI3BKI3NuW0ZL28/3Dcbcr97a9cG7v/dw+H8k74Z7zOYf3/XBuX5zbc+9xGWOMAACwREq8GwAAIBoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKqPj3YAT+vv7deLECY0dO1Yulyve7QAAomSMUWdnp7xer1JSBj6nSorgOnHihHJycuLdBgDgCh07dkwTJ04ccExSBNfYsWPj3QIAjEh+v9+R/QQCAeXk5Azp53lSBBdvDwJAfKSnpzu6v6H8POfiDACAVQguAIBVYhZcVVVVmjx5stLS0lRQUKADBw4MOP6NN97QzJkzlZaWptmzZ+udd96JVWsAAIvFJLi2bNmi1atXq6KiQg0NDcrLy1NxcbFOnToVdvz+/ftVWlqqFStWqLGxUUuWLNGSJUt0+PDhWLQHALCZiYH8/HxTXl4efNzX12e8Xq+prKwMO/6+++4zd955Z8iygoIC8+Mf/3hIf5/f7zeSKIqiqGEup1z8Oe73+wcd6/gZV29vrw4ePKiioqLgspSUFBUVFamuri7sNnV1dSHjJam4uDji+J6eHgUCgZACAIwMjgfX6dOn1dfXp8zMzJDlmZmZ8vl8Ybfx+XxRja+srJTH4wkWHz4GgJHDyqsK165dK7/fH6xjx47FuyUAwDBx/API48aN06hRo9TR0RGyvKOjQ1lZWWG3ycrKimq82+2W2+12pmEAgFUcP+NKTU3V3LlzVVtbG1zW39+v2tpaFRYWht2msLAwZLwk7dixI+J4AMAI5tglIf/jtddeM26321RXV5uPP/7YPPLIIyYjI8P4fD5jjDE//OEPzc9//vPg+Pfff9+MHj3aPP300+aTTz4xFRUVZsyYMebQoUND+vu4qpCiKCo+5ZRoriqMSXAZY8yGDRvMpEmTTGpqqsnPzzf19fXBdbfddpspKysLGf/666+b6dOnm9TUVDNr1izz9ttvD/nvIrgoiqLiU06JJrhcxhgjywUCAXk8nni3AQAjjlMRcvHnuN/vH/SLe628qhAAMHIlxW1NAMSHU//bTvZbEyXBG1sRxePfjjMuAIBVCC4AgFUILgCAVQguAIBVCC4AgFUILgCAVQguAIBVCC4AgFUILgCAVQguAIBVCC4AgFUILgCAVQguAIBVCC4AgFUILgCAVQguAIBVCC4AgFUILgCAVUbHuwEAw8vJ28jH47btg3Hy+TnFqXlKxOcWD5xxAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKzieHBVVlbq61//usaOHasJEyZoyZIlam5uHnCb6upquVyukEpLS3O6NQBAEnA8uPbs2aPy8nLV19drx44dunDhghYsWKDu7u4Bt0tPT9fJkyeD1dbW5nRrAIAk4PiNJLdv3x7yuLq6WhMmTNDBgwf1rW99K+J2LpdLWVlZTrcDAEgyMb8Dst/vlyRde+21A47r6upSbm6u+vv79bWvfU2//e1vNWvWrLBje3p61NPTE3wcCAScaxhIUIl499tE7Im7Mg9NIs7TUMX04oz+/n49+uijuuWWW3TTTTdFHDdjxgxt2rRJ27Zt08svv6z+/n7NmzdPx48fDzu+srJSHo8nWDk5ObF6CgCABOMyMfyvwMqVK/WXv/xF+/bt08SJE4e83YULF/TlL39ZpaWleuKJJy5ZH+6Mi/BCskvE/7UnokQ8k0jEf7tEnCfp83fp0tPTBxwTs7cKV61apbfeekt79+6NKrQkacyYMbr55pvV0tISdr3b7Zbb7XaiTQCAZRx/q9AYo1WrVmnr1q167733NGXKlKj30dfXp0OHDik7O9vp9gAAlnP8jKu8vFyvvPKKtm3bprFjx8rn80mSPB6PrrrqKknSsmXLdP3116uyslKS9Jvf/Ebf+MY3dMMNN+js2bNav3692tra9NBDDzndHgDAco4H18aNGyVJ//d//xeyfPPmzVq+fLkkqb29XSkp/z3Z++yzz/Twww/L5/Ppmmuu0dy5c7V//3595Stfcbo9AIDlYnpxxnAJBALyeDzxbgOIqSR4qQ6LRLzoIBH/7RJxnqShXZzBdxUCAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArBKz+3EBNkvE75ZLRIn6fXfJivn+HGdcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKtwB2RclmS/Q3Ai3mnWqTlPxOeWiJw8xplzZ3HGBQCwCsEFALAKwQUAsArBBQCwCsEFALCK48G1bt06uVyukJo5c+aA27zxxhuaOXOm0tLSNHv2bL3zzjtOtwUASBIxOeOaNWuWTp48Gax9+/ZFHLt//36VlpZqxYoVamxs1JIlS7RkyRIdPnw4Fq0BACznMg5/IGfdunWqqalRU1PTkMYvXbpU3d3deuutt4LLvvGNb+irX/2qnn322SHtIxAIyOPxXE67uEx8jmv48Tmu4cXnuOLD7/crPT19wDExOeM6cuSIvF6vpk6dqgceeEDt7e0Rx9bV1amoqChkWXFxserq6iJu09PTo0AgEFIAgJHB8eAqKChQdXW1tm/fro0bN6q1tVW33nqrOjs7w473+XzKzMwMWZaZmSmfzxfx76isrJTH4wlWTk6Oo88BAJC4HA+ukpIS3XvvvZozZ46Ki4v1zjvv6OzZs3r99dcd+zvWrl0rv98frGPHjjm2bwBAYov5dxVmZGRo+vTpamlpCbs+KytLHR0dIcs6OjqUlZUVcZ9ut1tut9vRPgEAdoj557i6urp09OhRZWdnh11fWFio2trakGU7duxQYWFhrFsDAFjI8eB67LHHtGfPHn366afav3+/7r77bo0aNUqlpaWSpGXLlmnt2rXB8T/5yU+0fft2/f73v9c///lPrVu3Th9++KFWrVrldGsAgCTg+FuFx48fV2lpqc6cOaPx48frm9/8purr6zV+/HhJUnt7u1JS/puX8+bN0yuvvKJf/vKX+sUvfqEbb7xRNTU1uummm5xuDQCQBBz/HFc88Dmu4ZcEh82AEvFzN3yOa3jxOa74iNvnuAAAiBWCCwBglZhfDo8rl4hvy/HWBxJVIr5e4CzOuAAAViG4AABWIbgAAFYhuAAAViG4AABWIbgAAFYhuAAAViG4AABWIbgAAFYhuAAAViG4AABWIbgAAFYhuAAAViG4AABWIbgAAFYhuAAAViG4AABW4Q7IX5CId0/lbsP2cvJ44jgYXsx34uKMCwBgFYILAGAVggsAYBWCCwBgFYILAGAVggsAYBWCCwBgFYILAGAVggsAYBWCCwBgFYILAGAVggsAYBWCCwBgFYILAGAVx4Nr8uTJcrlcl1R5eXnY8dXV1ZeMTUtLc7otAECScPx+XH//+9/V19cXfHz48GF9+9vf1r333htxm/T0dDU3Nwcfcx8cAEAkjgfX+PHjQx4/+eSTmjZtmm677baI27hcLmVlZTndCgAgCcX0d1y9vb16+eWX9eCDDw54FtXV1aXc3Fzl5ORo8eLF+uijj2LZFgDAYo6fcf2vmpoanT17VsuXL484ZsaMGdq0aZPmzJkjv9+vp59+WvPmzdNHH32kiRMnht2mp6dHPT09wceBQECS5Pf7lZ6efkU98zYlMPyMMY7ti9fwCGBiaMGCBea73/1uVNv09vaaadOmmV/+8pcRx1RUVBhJl5Tf77/SlsPul6Iut5wU7+fCPFHDUUP5OR6ztwrb2tq0c+dOPfTQQ1FtN2bMGN18881qaWmJOGbt2rXy+/3BOnbs2JW2CwCwRMyCa/PmzZowYYLuvPPOqLbr6+vToUOHlJ2dHXGM2+1Wenp6SAEARoaYBFd/f782b96ssrIyjR4d+mu0ZcuWae3atcHHv/nNb/TXv/5V//rXv9TQ0KAf/OAHamtri/pMDQAwMsTk4oydO3eqvb1dDz744CXr2tvblZLy37z87LPP9PDDD8vn8+maa67R3LlztX//fn3lK1+JRWsAAMu5/vPLTKsFAgF5PB6uKkTCcfLllczHJvOEi4byc5zvKgQAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYJaZ3QB5uHo8n3i0gSSTBV3gOC6fmie8XRDQ44wIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFhldLwbAJJZIt6S3hgT7xaAK8IZFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqUQfX3r17tWjRInm9XrlcLtXU1ISsN8boV7/6lbKzs3XVVVepqKhIR44cGXS/VVVVmjx5stLS0lRQUKADBw5E2xoAYASIOri6u7uVl5enqqqqsOt/97vf6Y9//KOeffZZffDBB7r66qtVXFys8+fPR9znli1btHr1alVUVKihoUF5eXkqLi7WqVOnom0PAJDszBWQZLZu3Rp83N/fb7Kyssz69euDy86ePWvcbrd59dVXI+4nPz/flJeXBx/39fUZr9drKisrh9SH3+83kijKsXJKvJ9HLJ+bk+I9J1TilN/vH/R4cfR3XK2trfL5fCoqKgou83g8KigoUF1dXdhtent7dfDgwZBtUlJSVFRUFHGbnp4eBQKBkAIAjAyOBpfP55MkZWZmhizPzMwMrvui06dPq6+vL6ptKisr5fF4gpWTk+NA9wAAG1h5VeHatWvl9/uDdezYsXi3BAAYJo4GV1ZWliSpo6MjZHlHR0dw3ReNGzdOo0aNimobt9ut9PT0kAIAjAyOBteUKVOUlZWl2tra4LJAIKAPPvhAhYWFYbdJTU3V3LlzQ7bp7+9XbW1txG0AACNYtFf/dHZ2msbGRtPY2GgkmT/84Q+msbHRtLW1GWOMefLJJ01GRobZtm2b+cc//mEWL15spkyZYv79738H93HHHXeYDRs2BB+/9tprxu12m+rqavPxxx+bRx55xGRkZBifzzeknriqkHK6nBLv5xHL5+akeM8JlTg1lKsKoz6Kd+3aFfYvKysrM8Z8fkn8448/bjIzM43b7Tbz5883zc3NIfvIzc01FRUVIcs2bNhgJk2aZFJTU01+fr6pr68fck8EF+V0OSXezyOWz81J8Z4TKnFqKMHl+s9BY7VAICCPxxPvNpBEnHpZcD+uoUnEeUJ8+P3+Qa9bsPKqQgDAyMUdkAFcNs6UEA+ccQEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKxCcAEArEJwAQCsQnABAKwyOt4NAE4xxsS7hZhy6vm5XC5H9gPEC2dcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKtEHVx79+7VokWL5PV65XK5VFNTE1x34cIFrVmzRrNnz9bVV18tr9erZcuW6cSJEwPuc926dXK5XCE1c+bMqJ8MACD5RR1c3d3dysvLU1VV1SXrzp07p4aGBj3++ONqaGjQm2++qebmZt11112D7nfWrFk6efJksPbt2xdtawCAESDqG0mWlJSopKQk7DqPx6MdO3aELHvmmWeUn5+v9vZ2TZo0KXIjo0crKysr2nYAACNMzO+A7Pf75XK5lJGRMeC4I0eOyOv1Ki0tTYWFhaqsrIwYdD09Perp6Qk+DgQCTrYMOHaXYCfvysydi4HPxfTijPPnz2vNmjUqLS1Venp6xHEFBQWqrq7W9u3btXHjRrW2turWW29VZ2dn2PGVlZXyeDzBysnJidVTAAAkGnMFJJmtW7eGXdfb22sWLVpkbr75ZuP3+6Pa72effWbS09PN888/H3b9+fPnjd/vD9axY8eMJGqEl5OSuSeKSuQaSl7E5K3CCxcu6L777lNbW5vee++9Ac+2wsnIyND06dPV0tISdr3b7Zbb7XaiVQCAZRx/q/BiaB05ckQ7d+7UddddF/U+urq6dPToUWVnZzvdHgDAclEHV1dXl5qamtTU1CRJam1tVVNTk9rb23XhwgV9//vf14cffqg///nP6uvrk8/nk8/nU29vb3Af8+fP1zPPPBN8/Nhjj2nPnj369NNPtX//ft19990aNWqUSktLr/wZAgCSS7Tvs+/atSvs+5JlZWWmtbU14vuWu3btCu4jNzfXVFRUBB8vXbrUZGdnm9TUVHP99debpUuXmpaWliH35Pf74/6+LBX/clIy90RRiVxD+R2X6z8vCKsFAgF5PJ54t4E4c/JQ5nJ4ID78fv+g10XwXYUAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAq8TkflyA7ZLgKzyBpMUZFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAq3AEZcefU3YZdLpcj+5ESsycAn+OMCwBgFYILAGAVggsAYBWCCwBgFYILAGCVqINr7969WrRokbxer1wul2pqakLWL1++XC6XK6QWLlw46H6rqqo0efJkpaWlqaCgQAcOHIi2NQDACBB1cHV3dysvL09VVVURxyxcuFAnT54M1quvvjrgPrds2aLVq1eroqJCDQ0NysvLU3FxsU6dOhVtewCAZGeugCSzdevWkGVlZWVm8eLFUe0nPz/flJeXBx/39fUZr9drKisrh7S93+83kihLyynJ3hNFjYTy+/2Dvq5i8juu3bt3a8KECZoxY4ZWrlypM2fORBzb29urgwcPqqioKLgsJSVFRUVFqqurC7tNT0+PAoFASAEARgbHg2vhwoV66aWXVFtbq6eeekp79uxRSUmJ+vr6wo4/ffq0+vr6lJmZGbI8MzNTPp8v7DaVlZXyeDzBysnJcfppAAASlONf+XT//fcH/zx79mzNmTNH06ZN0+7duzV//nxH/o61a9dq9erVwceBQIDwAoARIuaXw0+dOlXjxo1TS0tL2PXjxo3TqFGj1NHREbK8o6NDWVlZYbdxu91KT08PKQDAyBDz4Dp+/LjOnDmj7OzssOtTU1M1d+5c1dbWBpf19/ertrZWhYWFsW4PAGCZqIOrq6tLTU1NampqkiS1traqqalJ7e3t6urq0s9+9jPV19fr008/VW1trRYvXqwbbrhBxcXFwX3Mnz9fzzzzTPDx6tWr9ac//UkvvviiPvnkE61cuVLd3d360Y9+dOXPEACQXKK9vHfXrl1hL2EsKysz586dMwsWLDDjx483Y8aMMbm5uebhhx82Pp8vZB+5ubmmoqIiZNmGDRvMpEmTTGpqqsnPzzf19fVD7onL4e0upyR7TxQ1Emool8O7/vPislogEJDH44l3G7hMTh2C3I8LsJ/f7x/0ugW+qxAAYBWCCwBgFcc/xwXEi5PvevMWH5C4OOMCAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIU7IOOyOHm3YQCIBmdcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKtEHVx79+7VokWL5PV65XK5VFNTE7Le5XKFrfXr10fc57p16y4ZP3PmzKifDAAg+UUdXN3d3crLy1NVVVXY9SdPngypTZs2yeVy6Xvf+96A+501a1bIdvv27Yu2NQDACBD1jSRLSkpUUlIScX1WVlbI423btun222/X1KlTB25k9OhLtgUA4Iti+juujo4Ovf3221qxYsWgY48cOSKv16upU6fqgQceUHt7e8SxPT09CgQCIQUAGBliGlwvvviixo4dq3vuuWfAcQUFBaqurtb27du1ceNGtba26tZbb1VnZ2fY8ZWVlfJ4PMHKycmJRfuwTKTfr15OAUhg5gpIMlu3bo24fsaMGWbVqlVR7/ezzz4z6enp5vnnnw+7/vz588bv9wfr2LFjRhI1jJWI4j0nFEVdefn9/kFf61H/jmuo/va3v6m5uVlbtmyJetuMjAxNnz5dLS0tYde73W653e4rbREAYKGYvVX4wgsvaO7cucrLy4t6266uLh09elTZ2dkx6AwAYLOog6urq0tNTU1qamqSJLW2tqqpqSnkYopAIKA33nhDDz30UNh9zJ8/X88880zw8WOPPaY9e/bo008/1f79+3X33Xdr1KhRKi0tjbY9AECSi/qtwg8//FC333578PHq1aslSWVlZaqurpYkvfbaazLGRAyeo0eP6vTp08HHx48fV2lpqc6cOaPx48frm9/8purr6zV+/Pho2wMAJDnXf36pbbVAICCPxxPvNkaURDxsuBoQsJ/f71d6evqAY/iuQgCAVQguAIBVCC4AgFUILgCAVQguAIBVCC4AgFUILgCAVQguAIBVCC4AgFUILgCAVQguAIBVYnY/LmCo+I5BANHgjAsAYBWCCwBgFYILAGAVggsAYBWCCwBgFYILAGAVggsAYBWCCwBgFYILAGAVggsAYBWCCwBgFYILAGAVggsAYBWCCwBgFYILAGAVggsAYBWCCwBglaS4A7IxJt4tjDiBQCDeLQBIQkP5eZ4UwdXZ2RnvFkYcj8cT7xYAJKHOzs5Bf764TBKcrvT39+vEiRMaO3asXC5XxHGBQEA5OTk6duyY0tPTh7HDK0Pfw8vWviV7e6fv4ZWIfRtj1NnZKa/Xq5SUgX+LlRRnXCkpKZo4ceKQx6enpyfMP1Y06Ht42dq3ZG/v9D28Eq3vob6Tw8UZAACrEFwAAKuMqOByu92qqKiQ2+2OdytRoe/hZWvfkr290/fwsrXvi5Li4gwAwMgxos64AAD2I7gAAFYhuAAAViG4AABWSbrgqqqq0uTJk5WWlqaCggIdOHBgwPFvvPGGZs6cqbS0NM2ePVvvvPPOMHX6ucrKSn3961/X2LFjNWHCBC1ZskTNzc0DblNdXS2XyxVSaWlpw9Tx59atW3dJDzNnzhxwm3jPtSRNnjz5kr5dLpfKy8vDjo/nXO/du1eLFi2S1+uVy+VSTU1NyHpjjH71q18pOztbV111lYqKinTkyJFB9xvta8TJvi9cuKA1a9Zo9uzZuvrqq+X1erVs2TKdOHFiwH1ezvHmZN+StHz58kt6WLhw4aD7jed8Swp7vLtcLq1fvz7iPodjvq9EUgXXli1btHr1alVUVKihoUF5eXkqLi7WqVOnwo7fv3+/SktLtWLFCjU2NmrJkiVasmSJDh8+PGw979mzR+Xl5aqvr9eOHTt04cIFLViwQN3d3QNul56erpMnTwarra1tmDr+r1mzZoX0sG/fvohjE2GuJenvf/97SM87duyQJN17770Rt4nXXHd3dysvL09VVVVh1//ud7/TH//4Rz377LP64IMPdPXVV6u4uFjnz5+PuM9oXyNO933u3Dk1NDTo8ccfV0NDg9588001NzfrrrvuGnS/0RxvTvd90cKFC0N6ePXVVwfcZ7znW1JIvydPntSmTZvkcrn0ve99b8D9xnq+r4hJIvn5+aa8vDz4uK+vz3i9XlNZWRl2/H333WfuvPPOkGUFBQXmxz/+cUz7HMipU6eMJLNnz56IYzZv3mw8Hs/wNRVGRUWFycvLG/L4RJxrY4z5yU9+YqZNm2b6+/vDrk+EuTbGGElm69atwcf9/f0mKyvLrF+/Prjs7Nmzxu12m1dffTXifqJ9jTjddzgHDhwwkkxbW1vEMdEeb1cqXN9lZWVm8eLFUe0nEed78eLF5o477hhwzHDPd7SS5oyrt7dXBw8eVFFRUXBZSkqKioqKVFdXF3aburq6kPGSVFxcHHH8cPD7/ZKka6+9dsBxXV1dys3NVU5OjhYvXqyPPvpoONoLceTIEXm9Xk2dOlUPPPCA2tvbI45NxLnu7e3Vyy+/rAcffHDAL2dOhLn+otbWVvl8vpA59Xg8KigoiDinl/MaGQ5+v18ul0sZGRkDjovmeIuV3bt3a8KECZoxY4ZWrlypM2fORBybiPPd0dGht99+WytWrBh0bCLMdyRJE1ynT59WX1+fMjMzQ5ZnZmbK5/OF3cbn80U1Ptb6+/v16KOP6pZbbtFNN90UcdyMGTO0adMmbdu2TS+//LL6+/s1b948HT9+fNh6LSgoUHV1tbZv366NGzeqtbVVt956a8RbzCTaXEtSTU2Nzp49q+XLl0cckwhzHc7FeYtmTi/nNRJr58+f15o1a1RaWjrgl71Ge7zFwsKFC/XSSy+ptrZWTz31lPbs2aOSkhL19fWFHZ+I8/3iiy9q7NixuueeewYclwjzPZCk+Hb4ZFFeXq7Dhw8P+l5yYWGhCgsLg4/nzZunL3/5y3ruuef0xBNPxLpNSVJJSUnwz3PmzFFBQYFyc3P1+uuvD+l/c4nghRdeUElJibxeb8QxiTDXyerChQu67777ZIzRxo0bBxybCMfb/fffH/zz7NmzNWfOHE2bNk27d+/W/Pnzh6WHK7Vp0yY98MADg15glAjzPZCkOeMaN26cRo0apY6OjpDlHR0dysrKCrtNVlZWVONjadWqVXrrrbe0a9euqG7RIkljxozRzTffrJaWlhh1N7iMjAxNnz49Yg+JNNeS1NbWpp07d+qhhx6KartEmGtJwXmLZk4v5zUSKxdDq62tTTt27Ij61hqDHW/DYerUqRo3blzEHhJpviXpb3/7m5qbm6M+5qXEmO//lTTBlZqaqrlz56q2tja4rL+/X7W1tSH/Y/5fhYWFIeMlaceOHRHHx4IxRqtWrdLWrVv13nvvacqUKVHvo6+vT4cOHVJ2dnYMOhyarq4uHT16NGIPiTDX/2vz5s2aMGGC7rzzzqi2S4S5lqQpU6YoKysrZE4DgYA++OCDiHN6Oa+RWLgYWkeOHNHOnTt13XXXRb2PwY634XD8+HGdOXMmYg+JMt8XvfDCC5o7d67y8vKi3jYR5jtEvK8OcdJrr71m3G63qa6uNh9//LF55JFHTEZGhvH5fMYYY374wx+an//858Hx77//vhk9erR5+umnzSeffGIqKirMmDFjzKFDh4at55UrVxqPx2N2795tTp48Gaxz584Fx3yx71//+tfm3XffNUePHjUHDx40999/v0lLSzMfffTRsPX905/+1Ozevdu0traa999/3xQVFZlx48aZU6dOhe05Eeb6or6+PjNp0iSzZs2aS9Yl0lx3dnaaxsZG09jYaCSZP/zhD6axsTF49d2TTz5pMjIyzLZt28w//vEPs3jxYjNlyhTz73//O7iPO+64w2zYsCH4eLDXSKz77u3tNXfddZeZOHGiaWpqCjnme3p6IvY92PEW6747OzvNY489Zurq6kxra6vZuXOn+drXvmZuvPFGc/78+Yh9x3u+L/L7/eZLX/qS2bhxY9h9xGO+r0RSBZcxxmzYsMFMmjTJpKammvz8fFNfXx9cd9ttt5mysrKQ8a+//rqZPn26SU1NNbNmzTJvv/32sPYrKWxt3rw5Yt+PPvpo8DlmZmaa73znO6ahoWFY+166dKnJzs42qamp5vrrrzdLly41LS0tEXs2Jv5zfdG7775rJJnm5uZL1iXSXO/atSvssXGxv/7+fvP444+bzMxM43a7zfz58y95Trm5uaaioiJk2UCvkVj33draGvGY37VrV8S+BzveYt33uXPnzIIFC8z48ePNmDFjTG5urnn44YcvCaBEm++LnnvuOXPVVVeZs2fPht1HPOb7SnBbEwCAVZLmd1wAgJGB4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBY5f8BQfLQPDoP0c4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test data point\n",
    "num = np.random.randint(0, 10)\n",
    "print(\"Number\", num)\n",
    "sample = np.random.randint(1, 101)\n",
    "fname = f'data/stroke_{num}_{str(0)*(4-len(str(sample))) + str(sample)}.csv'\n",
    "\n",
    "data = pd.read_csv(fname).to_numpy()\n",
    "data = preprocess_data(data)\n",
    "M = image_creation(data)\n",
    "plt.imshow(M, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = open('data.csv', 'w')\n",
    "for num in range(0, 10):\n",
    "    for sample in range(1,101):\n",
    "        fname = f'data/stroke_{num}_{str(0)*(4-len(str(sample))) + str(sample)}.csv'\n",
    "        data = pd.read_csv(fname).to_numpy()\n",
    "        data = preprocess_data(data)\n",
    "        M = image_creation(data)\n",
    "\n",
    "        features = np.ravel(M)\n",
    "        # for f in feature_extraction(data):\n",
    "        #     features = np.append(features, f)\n",
    "        for f in features:\n",
    "            data_file.write(str(f) + ',')\n",
    "        data_file.write(str(num) + '\\n')\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 625 into shape (16,16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m fvalidation \u001b[38;5;241m=\u001b[39m fdata[train_size:train_size\u001b[38;5;241m+\u001b[39mvalidation_size]\n\u001b[0;32m      8\u001b[0m ftest \u001b[38;5;241m=\u001b[39m fdata[train_size\u001b[38;5;241m+\u001b[39mvalidation_size:]\n\u001b[1;32m---> 10\u001b[0m X_train \u001b[38;5;241m=\u001b[39m [\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mftrain\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ftrain))]\n\u001b[0;32m     11\u001b[0m X_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_train)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     12\u001b[0m y_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(ftrain[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mlong()\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:299\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreshape\u001b[39m(a, newshape, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    216\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;124;03m           [5, 6]])\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreshape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 625 into shape (16,16)"
     ]
    }
   ],
   "source": [
    "fdata = pd.read_csv('data.csv', header=None).to_numpy()\n",
    "np.random.shuffle(fdata)\n",
    "\n",
    "train_size = int(0.7 * len(fdata))\n",
    "validation_size = int(0.1 * len(fdata))\n",
    "ftrain = fdata[:train_size]\n",
    "fvalidation = fdata[train_size:train_size+validation_size]\n",
    "ftest = fdata[train_size+validation_size:]\n",
    "\n",
    "X_train = [np.reshape(ftrain[i,:-1], (16, 16)) for i in range(len(ftrain))]\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(ftrain[:,-1]).long()\n",
    "\n",
    "X_validation = [np.reshape(fvalidation[i, :-1], (16, 16)) for i in range(len(fvalidation))]\n",
    "X_validation = torch.tensor(X_validation).float()\n",
    "y_validation = torch.tensor(fvalidation[:,-1]).long()\n",
    "#print(np.shape(ftest[0:-1]))\n",
    "X_test = [np.reshape(ftest[i,:-1], (16, 16)) for i in range(len(ftest))]\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(ftest[:,-1]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (132) to match target batch_size (0).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     34\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(X_train[i])\n\u001b[1;32m---> 35\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (132) to match target batch_size (0)."
     ]
    }
   ],
   "source": [
    "class NumberClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NumberClassifier, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(16, 33, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(33, 66, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = torch.nn.Linear(8, 16)\n",
    "        self.fc2 = torch.nn.Linear(16, 16)\n",
    "        self.fc3 = torch.nn.Linear(16, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 16, 16, 1)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = x.view(-1, 8)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "model = NumberClassifier()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "J = []\n",
    "V = []\n",
    "\n",
    "best_validation_loss = 1000000\n",
    "for epoch in range(10):\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i in range(len(X_train)):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train[i])\n",
    "        loss = criterion(outputs, y_train[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(X_train)\n",
    "    J.append(avg_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        voutputs = model(X_validation)\n",
    "        vloss = criterion(voutputs, y_validation)\n",
    "        validation_loss = vloss.item()\n",
    "        V.append(validation_loss)\n",
    "    \n",
    "    if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch}, loss: {running_loss}, validation loss: {validation_loss}')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(J, label='Training loss')\n",
    "plt.plot(V, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.0\n",
      "[[17.  0.  0.  0.  0.  1.  1.  0.  0.  0.]\n",
      " [ 1. 19.  1.  0.  0.  2.  1.  1.  0.  1.]\n",
      " [ 0.  0. 16.  3.  0.  1.  1.  0.  2.  0.]\n",
      " [ 0.  0.  0. 10.  0.  0.  0.  0.  4.  0.]\n",
      " [ 0.  0.  0.  0. 17.  0.  0.  0.  0.  1.]\n",
      " [ 6.  0.  1.  1.  0. 10.  2.  0.  0.  0.]\n",
      " [ 5.  0.  0.  0.  1.  3. 13.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0. 16.  0.  3.]\n",
      " [ 1.  0.  1.  2.  0.  3.  0.  0. 11.  1.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0. 17.]]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model.pth', weights_only=False))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = torch.max(outputs, dim=1)\n",
    "    total += y_test.size(0)\n",
    "    correct += (predicted == y_test).sum().item()\n",
    "    print(f'Accuracy: {100 * correct / total}')\n",
    "\n",
    "    # Confusion matrix\n",
    "    confusion_matrix = np.zeros((10, 10))\n",
    "    for i in range(len(predicted)):\n",
    "        confusion_matrix[y_test[i], predicted[i]] += 1\n",
    "    print(confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
